{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import codecs\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Training Data, Dev Data and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Loads the data stored at `data_path`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        data_path : str\n",
    "        \n",
    "    Yields\n",
    "    -------\n",
    "        word : list[unicode]\n",
    "\n",
    "        score : int\n",
    "    \"\"\"\n",
    "    with codecs.open(data_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            words_score = line.split(\"|\")\n",
    "            words = words_score[:-1]\n",
    "            score = int(words_score[-1])\n",
    "            yield words, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_sentences_path = path.join(parent_dir, \"data/sst/train_sentences.txt\")\n",
    "train_phrases_path = path.join(parent_dir, \"data/sst/train_phrases.txt\")\n",
    "dev_full_sentences_path = path.join(parent_dir, \"data/sst/dev_sentences.txt\")\n",
    "test_full_sentences_path = path.join(parent_dir, \"data/sst/test_sentences.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_sentences, train_full_sentences_scores = zip(*load_data(train_full_sentences_path))\n",
    "train_phrases, train_phrases_scores = zip(*load_data(train_phrases_path))\n",
    "train_sentences = train_full_sentences + train_phrases\n",
    "train_scores = train_full_sentences_scores + train_phrases_scores\n",
    "\n",
    "dev_full_sentences, dev_full_sentences_scores = zip(*load_data(dev_full_sentences_path))\n",
    "test_full_sentences, test_full_sentences_scores = zip(*load_data(test_full_sentences_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example sentences and score from the training data\n",
    "* 0 - most negative\n",
    "* 2 - neutral\n",
    "* 4 - most positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, score in zip(train_full_sentences[:5], train_full_sentences_scores[:5]):\n",
    "    print(\" \".join(sentence) + \" | \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = path.join(parent_dir, \n",
    "                           \"data/sst/word2vec_filtered_lower-negative300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero vector for padding sentence to a fixed length and for unknown words\n",
    "weight_vectors = [np.zeros(300, dtype=np.float32)]\n",
    "word2idx = {u\"<unk>\" : 0}\n",
    "\n",
    "with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "    print('loading word2vec embeddings from %s' % embedding_path)\n",
    "    for line in f:\n",
    "        word, vec = line.split(u' ', 1)\n",
    "        word2idx[word] = len(weight_vectors)\n",
    "        weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "\n",
    "# Random embedding vector for filter padding.\n",
    "word2idx[u\"<filter_padding>\"] = len(weight_vectors)\n",
    "weight_vectors.append(np.random.uniform(-0.25, 0.25, 300).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = np.asarray(weight_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transform phrases/sentences to embedding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequences(\n",
    "    [words_to_embedding_index_with_padding(words, word2idx) for words in train_sentences],\n",
    "    word2idx[u'<unk>']\n",
    ")\n",
    "\n",
    "dev_data = pad_sequences(\n",
    "    [words_to_embedding_index_with_padding(words, word2idx) for words in dev_full_sentences],\n",
    "    word2idx[u'<unk>']\n",
    ")\n",
    "\n",
    "test_data = pad_sequences(\n",
    "    [words_to_embedding_index_with_padding(words, word2idx) for words in test_full_sentences],\n",
    "    word2idx[u'<unk>']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN\"\"\"\n",
    "    \n",
    "    # 1. Input Data\n",
    "    input_data = features[\"x\"]\n",
    "    \n",
    "    # 2. Word Embedding\n",
    "    channel = tf.contrib.layers.embed_sequence(\n",
    "        ids=input_data,\n",
    "        initializer=tf.constant_initializer(\n",
    "            value=we,\n",
    "            dtype=tf.float32),\n",
    "        trainable=False,\n",
    "        scope='embedding',\n",
    "        vocab_size=we.shape[0], \n",
    "        embed_dim=we.shape[1]\n",
    "    )\n",
    "    \n",
    "    # 3. Convolution\n",
    "    branches = []\n",
    "    for branch_index in range(3):\n",
    "        with tf.variable_scope('CNN_Layer' + str(branch_index)):\n",
    "            inference = tf.layers.conv1d(\n",
    "                channel,\n",
    "                filters=100,  # feature maps in the paper\n",
    "                kernel_size=3 + branch_index, # filter window\n",
    "                padding='VALID',\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_uniform_initializer(\n",
    "                                    minval=-0.01,\n",
    "                                    maxval=0.01,\n",
    "                                    dtype=tf.float32\n",
    "                                ),\n",
    "                bias_initializer=tf.zeros_initializer(dtype=tf.float32)\n",
    "            )\n",
    "            branch = tf.reduce_max(input_tensor=inference, axis=1)\n",
    "            branches.append(branch)\n",
    "    network = tf.concat(values=branches, axis=1)\n",
    "    \n",
    "    # 5. Dropout for penultimate layer\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=network, \n",
    "        rate=0.5, \n",
    "        training=mode == tf.estimator.ModeKeys.TRAIN\n",
    "    )\n",
    "    \n",
    "    # 6. Final layer\n",
    "    logits = tf.layers.dense(inputs=dropout,\n",
    "                             kernel_initializer=tf.random_normal_initializer(\n",
    "                                 mean=0.0, \n",
    "                                 stddev=0.01\n",
    "                             ),\n",
    "                             bias_initializer=tf.zeros_initializer(dtype=tf.float32),\n",
    "                             units=5)\n",
    "    \n",
    "    # Training and Prediction\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "  \n",
    "    # Return predictions if mode is to PREDICT\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=5)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdadeltaOptimizer(\n",
    "            learning_rate=1,\n",
    "            rho=0.95,\n",
    "            epsilon=1e-06\n",
    "        )\n",
    "        \n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step()\n",
    "        )\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, \n",
    "            predictions=predictions[\"classes\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape, dev_data.shape, test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.asarray(train_scores, dtype=np.int32)\n",
    "dev_labels = np.asarray(dev_full_sentences_scores, dtype=np.int32)\n",
    "test_labels = np.asarray(test_full_sentences_scores, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = tf.estimator.RunConfig(model_dir=\"../data/model/cnn/\",\n",
    "                                    save_checkpoints_secs=600,\n",
    "                                    log_step_count_steps=1000,\n",
    "                                    save_summary_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                        config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 1\n",
    "best_dev_score = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(num_runs):\n",
    "    # train\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_data},\n",
    "        y=train_labels,\n",
    "        batch_size=50,\n",
    "        num_epochs=None,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    cnn_classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=3356\n",
    "    )\n",
    "    \n",
    "    # Dev scores\n",
    "    dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={'x': dev_data},\n",
    "        y=dev_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False\n",
    "    )\n",
    "    dev_score = cnn_classifier.evaluate(input_fn=dev_input_fn)['accuracy']\n",
    "    print('Accuracy: {0:f}'.format(dev_score))\n",
    "    \n",
    "    \n",
    "    # Test scores\n",
    "    test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={'x': test_data},\n",
    "      y=test_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False\n",
    "    )\n",
    "    test_score = cnn_classifier.evaluate(input_fn=test_input_fn)['accuracy']\n",
    "    print('Accuracy: {0:f}'.format(test_score))\n",
    "    \n",
    "    if dev_score > best_dev_score:\n",
    "        dev_score = dev_score\n",
    "        best_model_test_score = test_score\n",
    "        \n",
    "    print(50 * '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_score, test_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
