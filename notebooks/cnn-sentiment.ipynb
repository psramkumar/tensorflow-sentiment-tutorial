{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import codecs\n",
    "from collections import namedtuple\n",
    "import os\n",
    "from os import path\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_dir = path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Training Data, Dev Data and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    \"\"\"Loads the data stored at `data_path`.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        data_path : str\n",
    "        \n",
    "    Yields\n",
    "    -------\n",
    "        word : list[unicode]\n",
    "\n",
    "        score : int\n",
    "    \"\"\"\n",
    "    with codecs.open(data_path, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.lower()\n",
    "            words_score = line.split(\"|\")\n",
    "            words = words_score[:-1]\n",
    "            score = int(words_score[-1])\n",
    "            yield words, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_full_sentences_path = path.join(parent_dir, \"data/sst/train_sentences.txt\")\n",
    "train_full_sentences, train_full_sentences_scores = zip(*load_data(train_full_sentences_path))\n",
    "\n",
    "train_phrases_path = path.join(parent_dir, \"data/sst/train_phrases.txt\")\n",
    "train_phrases, train_phrases_scores = zip(*load_data(train_phrases_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_full_sentences + train_phrases\n",
    "train_scores = train_full_sentences_scores + train_phrases_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_full_sentences_path = path.join(parent_dir, \"data/sst/dev_sentences.txt\")\n",
    "dev_full_sentences, dev_full_sentences_scores = zip(*load_data(dev_full_sentences_path))\n",
    "\n",
    "test_full_sentences_path = path.join(parent_dir, \"data/sst/test_sentences.txt\")\n",
    "test_full_sentences, test_full_sentences_scores = zip(*load_data(test_full_sentences_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some example sentences and score from the training data\n",
    "* 0 - most negative\n",
    "* 2 - neutral\n",
    "* 4 - most positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example sentences and scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence, score in zip(train_full_sentences[:10], train_full_sentences_scores[:10]):\n",
    "    print(\" \".join(sentence) + \" | \" + str(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = path.join(parent_dir, \n",
    "                           \"data/sst/word2vec_filtered_lower-negative300.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero vector for padding sentence to a fixed length and for unknown words\n",
    "weight_vectors = [np.zeros(300, dtype=np.float32)]\n",
    "word2idx = {u\"<unk>\" : 0}\n",
    "\n",
    "with codecs.open(embedding_path, encoding='utf-8') as f:\n",
    "    print('loading word2vec embeddings from %s' % embedding_path)\n",
    "    for line in f:\n",
    "        word, vec = line.split(u' ', 1)\n",
    "        word2idx[word] = len(weight_vectors)\n",
    "        weight_vectors.append(np.array(vec.split(), dtype=np.float32))\n",
    "\n",
    "# Random embedding vector for filter padding.\n",
    "word2idx[u\"<filter_padding>\"] = len(weight_vectors)\n",
    "weight_vectors.append(np.random.uniform(-0.25, 0.25, 300).astype(np.float32))\n",
    "\n",
    "word2idx[u\".\"] = len(weight_vectors)\n",
    "weight_vectors.append(np.random.uniform(-0.25, 0.25, 300).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = np.asarray(weight_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transform phrases/sentences to embedding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pad_sequences(\n",
    "    [words_to_embedding_index_with_padding(words, word2idx) if len(words) >= 10\n",
    "     else words_to_embedding_index_with_padding(words, word2idx, filter_len=3)\n",
    "     for words in train_sentences],\n",
    "    word2idx[u'<unk>']\n",
    ")\n",
    "\n",
    "dev_data = pad_sequences(\n",
    "    [words_to_embedding_index_with_padding(words, word2idx) if len(words) >= 10\n",
    "     else words_to_embedding_index_with_padding(words, word2idx, filter_len=3)\n",
    "     for words in dev_full_sentences],\n",
    "    word2idx[u'<unk>']\n",
    ")\n",
    "\n",
    "test_data = pad_sequences(\n",
    "    [words_to_embedding_index_with_padding(words, word2idx) if len(words) >= 10\n",
    "     else words_to_embedding_index_with_padding(words, word2idx, filter_len=3)\n",
    "     for words in test_full_sentences],\n",
    "    word2idx[u'<unk>']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.asarray(train_scores, dtype=np.int32)\n",
    "dev_labels = np.asarray(dev_full_sentences_scores, dtype=np.int32)\n",
    "test_labels = np.asarray(test_full_sentences_scores, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "    \"\"\"Model function for CNN\"\"\"\n",
    "    \n",
    "    # 1. Input Data\n",
    "    input_data = features\n",
    "    \n",
    "    # 2. Word Embedding\n",
    "    channel = tf.contrib.layers.embed_sequence(\n",
    "        ids=input_data,\n",
    "        initializer=tf.constant_initializer(\n",
    "            value=we,\n",
    "            dtype=tf.float32),\n",
    "        trainable=True,\n",
    "        scope='embedding',\n",
    "        vocab_size=we.shape[0],\n",
    "        embed_dim=we.shape[1]\n",
    "    )\n",
    "    # 3. Dropout for input layer\n",
    "    chanel = tf.layers.dropout(\n",
    "        inputs=channel, \n",
    "        rate=0.5, \n",
    "        training=(mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    )\n",
    "    \n",
    "    # 4. Convolution\n",
    "    branches = []\n",
    "    for branch_index in range(3):\n",
    "        with tf.variable_scope('CNN_Layer' + str(branch_index)):\n",
    "            inference = tf.layers.conv1d(\n",
    "                channel,\n",
    "                filters=300,  # feature maps in the paper\n",
    "                kernel_size=3 + branch_index, # filter window\n",
    "                padding='VALID',\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_uniform_initializer(\n",
    "                                    minval=-0.01,\n",
    "                                    maxval=0.01,\n",
    "                                    dtype=tf.float32)\n",
    "            )\n",
    "            branch = tf.reduce_max(input_tensor=inference, axis=1)\n",
    "            branches.append(branch)\n",
    "    network = tf.concat(values=branches, axis=1)\n",
    "    \n",
    "    # 5. Dropout for penultimate layer\n",
    "    dropout = tf.layers.dropout(\n",
    "        inputs=network, \n",
    "        rate=0.5, \n",
    "        training=(mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    )\n",
    "    \n",
    "    # 6. Final layer\n",
    "    logits = tf.layers.dense(inputs=dropout,\n",
    "                             kernel_initializer=tf.random_normal_initializer(\n",
    "                                 mean=0.0,\n",
    "                                 stddev=0.01\n",
    "                             ),\n",
    "                             kernel_regularizer=tf.contrib.layers.l2_regularizer(0.001),\n",
    "                             units=5)\n",
    "    \n",
    "    # Predictions\n",
    "    predictions = {\n",
    "        # Generate predictions\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "  \n",
    "    # Return predictions if mode is to PREDICT\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=5)\n",
    "    loss = tf.losses.softmax_cross_entropy(onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdadeltaOptimizer(\n",
    "            learning_rate=0.1,\n",
    "            rho=0.95,\n",
    "            epsilon=1e-06\n",
    "        )\n",
    "        \n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step()\n",
    "        )\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, \n",
    "            predictions=predictions[\"classes\"]\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SetWeights(tf.train.SessionRunHook):\n",
    "    \"\"\"Hook to add ops to be executed after each call to run.\n",
    "    Resets embedding for u'<unk>' token to zeros. Clips the \n",
    "    norm of the weights for the punultimate fully connected layer.\n",
    "    \"\"\"\n",
    "    def begin(self):\n",
    "        fc = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='dense/kernel')[0]\n",
    "        embedding = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='embedding/embeddings')[0]\n",
    "        self.update_fc = fc.assign(\n",
    "            tf.clip_by_norm(t=fc,\n",
    "                            clip_norm=25,\n",
    "                            axes=[0])\n",
    "        )\n",
    "        self.update_we = tf.scatter_update(embedding,\n",
    "                                           [0],\n",
    "                                           tf.zeros((1, 300), dtype=tf.float32))\n",
    "    \n",
    "    def after_run(self, run_context, run_values):\n",
    "        run_context.session.run([self.update_fc, self.update_we])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrialResult = namedtuple('TrialResult', ['trial', 'best_run', 'best_dev_score', 'best_test_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn_trial(trial,\n",
    "                    train_input_fn,\n",
    "                    dev_input_fn,\n",
    "                    test_input_fn,\n",
    "                    model_dir=\"../data/model/cnn\"):\n",
    "    \"\"\"Runs one trial for the training of the cnn classifier.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trial: int\n",
    "    \n",
    "    train_input_fn: tensorflow.python.estimator.inputs.numpy_io.input_fn\n",
    "    \n",
    "    dev_input_fn: tensorflow.python.estimator.inputs.numpy_io.input_fn\n",
    "    \n",
    "    test_input_fn: tensorflow.python.estimator.inputs.numpy_io.input_fn\n",
    "    \n",
    "    model_dir: str\n",
    "    \"\"\"\n",
    "    run_config = tf.estimator.RunConfig(model_dir=model_dir + str(trial),\n",
    "                                        save_checkpoints_secs=600,\n",
    "                                        log_step_count_steps=1000,\n",
    "                                        save_summary_steps=1000,\n",
    "                                        session_config=tf.ConfigProto(log_device_placement=True))\n",
    "    cnn_classifier = tf.estimator.Estimator(model_fn=cnn_model_fn,\n",
    "                                            config=run_config)\n",
    "    set_weights_hook = SetWeights()\n",
    "    \n",
    "    best_dev_score = 0.0\n",
    "    best_test_score = 0.0\n",
    "    best_run = 0\n",
    "    \n",
    "    for run in range(50):\n",
    "        cnn_classifier.train(\n",
    "            input_fn=train_input_fn,\n",
    "            steps=1000,\n",
    "            hooks=[set_weights_hook]\n",
    "        )\n",
    "        \n",
    "        dev_score = cnn_classifier.evaluate(input_fn=dev_input_fn)['accuracy']\n",
    "        test_score = cnn_classifier.evaluate(input_fn=test_input_fn)['accuracy']\n",
    "        if dev_score > best_dev_score:\n",
    "            best_run = run\n",
    "            best_dev_score = dev_score\n",
    "            best_test_score = test_score\n",
    "            print(\"Best run: %d | Best dev score: %.4f | Best test score: %.4f\" \n",
    "                  % (best_run, best_dev_score, best_test_score))\n",
    "\n",
    "    return TrialResult(trial=trial,\n",
    "                       best_run=best_run,\n",
    "                       best_dev_score=best_dev_score,\n",
    "                       best_test_score=best_test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x=train_data,\n",
    "    y=train_labels,\n",
    "    batch_size=50,\n",
    "    num_epochs=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dev_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x=dev_data,\n",
    "    y=dev_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "  x=test_data,\n",
    "  y=test_labels,\n",
    "  num_epochs=1,\n",
    "  shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 5\n",
    "\n",
    "trial_results = []\n",
    "for trial in range(num_trials):\n",
    "    trial_results.append(train_cnn_trial(trial,\n",
    "                                         train_input_fn,\n",
    "                                         dev_input_fn,\n",
    "                                         test_input_fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = [trial_result.best_test_score for trial_result in trial_results]\n",
    "\n",
    "test_result = (\n",
    "    \"Number of trials: %d\\n\"\n",
    "    \"Mean test score: %.4f\\n\"\n",
    "    \"Standard deviation: %.4f\\n\"\n",
    "    \"Minimum test score: %.4f\\n\" \n",
    "    \"Maximum test score: %.4f\"\n",
    ") % (num_trials,\n",
    "     (100.0 * np.mean(test_results)),\n",
    "     (100.0 * np.std(test_results)),\n",
    "     (100.0 * np.min(test_results)),\n",
    "     (100.0 * np.max(test_results)))\n",
    "\n",
    "print(test_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
